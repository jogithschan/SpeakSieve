import React from 'react'
import './AboutProject.css'


const AboutProject = () => {
  return (
    
    <div class = "box">
      <div>
        <h1>About Project</h1>
      </div>
      <br></br>
      
      <div class = "content">
        <h3>Problem Statement</h3>
        <p>
        To extract any kind of information, such as context, topic, speakers, etc. from audios, one will have to listen to the entire audio, which is a time consuming and tiring process. It can be done by going through a series of steps - noise clearing, speaker separation, transcription, etc - but that requires some experience in using such tools. Even if a compact tool exists, it is paid or too complex to be used for quick work. 
We aim to ease the process of information retrieval from audios, by combining various steps and providing a number of basic features which will be user-friendly and handy for everyone. 

        </p>
      </div>
      <br></br>
      <div class = "content">
        <h3>The Idea</h3>
        <p>
        The idea of our web app is providing a single-source audio speaker separation which isolates a speakerâ€™s voice form a mixture of multiple speakers in a audio recording which is different from application services like moises as they separate vocals from music and other elements which are relatively distinct and can recognized as individual parts by the algorithms used. Alternatively, adobe podcast is more focused on a single use of studio-like recordings and noise separation by using the machine learning algorithm it has been trained on. 
        </p>

      </div>

      <br></br>


      <div class = "content">
        <h3>Literature Review</h3>
        <ol>
        <li> <b>Single-Channel Multi-Speaker Separation using Deep Clustering</b> - Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, John R. Hershey <a href="https://arxiv.org/abs/1607.02173" target="_blank">2016</a> </li>
        <li> <b>Deep attractor network for single-microphone speaker separation</b> - Zhuo Chen Yi Luo Nima Mesgarani <a href="https://arxiv.org/abs/1611.08930" target="_blank">2017</a>
 </li>
        <li> <b>VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking</b> - Wang, Q., Muckenhirn, H., Wilson, K., Sridhar, P., Wu, Z., Hershey, J., Saurous, R.A., Weiss, R.J., Jia, Y. and Moreno, I.L., <a href="https://arxiv.org/abs/1810.04826" target="_blank">2018</a>
 </li>
        <li> <b>Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation</b> - Yi Luo, Nima Mesgarani <a href="https://ieeexplore.ieee.org/document/8707065" target="_blank">2019</a> </li>
        <li> <b>Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM with Auxiliary Identity Loss</b> -  Shi, Z., Liu, R., & Han, J.  <a href="https://arxiv.org/abs/2008.03149" target="_blank">2020</a> </li>
        <li> <b>SFSRNet: Super-resolution for Single-Channel Audio Source Separation</b> - Joel Rixen, Matthias Renz <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21372" target="_blank">2022</a>
 </li>
</ol>
      </div>
      <br></br>
      <div class = "content">
        <h3>Our Progress</h3>
        <p>We went through a number of models for the purpose of speaker detection from audios. We tested and evaluated based upon how accurately they were able to segregate the speakers in a given audio. We tested on audios generated by group members itself.
<br></br>
        After going through a number of models, we selected Asteroid, a PyTorch-based audio source separation toolkit for our first step of separating speakers from given audio.
        It gave good accuracy for both, audio generated by us, and audio available online.
<br></br>
        Next step was to convert these clippings of individual users to text. We used the SpeechRecognition library of Python for performing speech recognition. Currently we are getting poor results for this step on using multi-speaker audios.
</p>
      </div>

    </div>
  
    

    
  )
}

export default AboutProject